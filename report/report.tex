\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
%\NOREVIEWERNOTES

\begin{document}
\title
    [Мультимоделирование как универсальный способ описания выборки общего вида] 
    {Мультимоделирование как универсальный способ описания выборки общего вида}
\author
    [Качанов~В.\,В.] % список авторов для колонтитула; не нужен, если основной список влезает в колонтитул
    {Качанов~В.\,В., Адуенко~А.\,А., Стрижов~В.\,В.} % основной список авторов, выводимый в оглавление
\email
    {kachanov.vv@phystech.edu}
\abstract
    {В данной работе рассматривается мультимоделирование как универсальный способ описания выборки общего вида. В работу входит построение метода инкрементального уточнения структуры мультимодели при появлении новых объектов. Для достижения поставленных целей предлагается использовать байесовский подход для выбора моделей на основании обоснованности. Новизна данной работы заключается в предложении метода построения оптимальной схемы обновления структуры мультимодели при появлении новых объектов. Достоверность результатов подтверждена экспериментальной проверкой полученных методов на реальных данных из репозитория UCI.}

\maketitle
\Russian
%\linenumbers
\section{Введение}
Решается задача инкрементального уточнения структуры мультимодели. 

\section{Постановка задачи}
Пусть имеется $K$ моделей, $k \in [1,K],~\{\bar{x}_i,y_i\}_{i=1}^m,~\text{- выборка}$
$$y_i = \bar{w}_k^T\bar{x}_i+\epsilon_i,~ \epsilon_i \sim \mathcal{N}(0,\beta^{-1})$$

$\text{Априорное распределение на}~ \bar{\pi}: p(\bar{\pi} |\mu) = Dir(\bar{\pi}|\mu)$\\
$\text{Априорное распределение моделей}: p(\bar{w}_k) = \mathcal{N}(\bar{w}_k|0,A_k)$\\
Совместное правдоподобие:
$$p(\bar{y},\bar{W},\bar{\pi}|X,A,\beta,\mu)=Dir(\bar{\pi}|\mu)\prod\limits_{k=1}^K \mathcal{N}(\bar{w}_k|0,A_k)\prod\limits_{i=1}^m  \sum\limits_{j=1}^K\pi_k \mathcal{N}(y_i|\bar{w}_k^T \bar x_i\bar{w}_k,\beta^{-1})$$\\
Апостериорное распределение пропорционально:
$$p(\bar{W},\bar{\pi}|X,\bar y,A,\beta,\mu)\sim \prod\limits_{i=1}^m\left(\sum\limits_{j=1}^K \pi_k exp\left(-\frac{\beta}{2}(y_i - \bar{w}_j^T\bar{x_i})^2\right)\right)*$$
$$*\prod\limits_{k=1}^K\pi_k^{\mu-1}exp(-0.5\bar{w}_k^TA_k\bar{w}_k)$$
Для решения задачи воспользуемся вариационным ЕМ-алгоритмом со скрытой переменной $Z = ||z_{ik}||$, тогда совместное правдоподобие перепишется в виде
$$p(\bar{y},\bar{W},\bar{\pi},Z|X,A,\beta,\mu)=Dir(\bar{\pi}|\mu)\prod\limits_{k=1}^K \mathcal{N}(\bar{w}_k|0,A_k)\prod\limits_{i=1}^m *$$
$$*\left( \prod\limits_{j=1}^K\pi_j \mathcal{N}(y_i|\bar{w}_k^T x_i\bar{w}_k,\beta^{-1})\right)^{z_{ij}}$$\\
Воспользовавшись вариационным приближением: ~$ q(\bar{\pi},Z,W) = q(\bar{\pi})~q(Z)~q(W)$\\

$$\log~q(\bar{\pi}) = \sum\limits_{k=1}^K \log~\pi_k\left( \sum\limits_{i=1}^m\mathbb{E}z_{ik}+\mu-1\right)$$
$$ \Rightarrow q(\bar{\pi}) = Dir(\bar{\pi}|\mu + \bar{\alpha}),~ \alpha_k=\sum\limits_{i=1}^m z_{ik}$$
$$log~q(W) \sim \sum\limits_{i=1}^m-\frac{1}{2}\bar{w}_k^TA_k\bar{w}_k + \sum\limits_{i=1}^m \sum\limits_{l=1}^K \mathbb{E}z_{il}\frac{\beta}{2}\left(\bar{w}_l^Tx_ix_i^TA_k\bar{w}_l - 2y_i\bar{w}_l^Tx_i\right)$$
$$q(\bar{w}_k) = \mathcal{N}(\bar{w}_k|m_k, \Sigma_k^{-1})$$
$$\log~q(Z) \sim \sum \limits_{k=1}^{K} \sum \limits_{i=1}^{m} z_{ik} \left(\mathbb{E} \log~\pi_k - \frac{\beta}{2}(y_i-\bar{w_k}^T \bar{x_i})^2 \right) \Rightarrow$$
$$\Rightarrow p(z_{ik} = 1) = C~exp \left(\mathbb{E} \log~\pi_k -  \frac{\beta}{2}(y_i-\bar{w_k}^T \bar{x_i})^2 \right)$$

$$\mathbb{E}_q log~p(\bar{y}, \bar{p}, W, Z|X, A, \beta, \mu) = \mathcal{F}(A, \beta) \propto$$
$$\sum \limits_{k = 1}^K ((\mu +2\alpha_k - 1)\mathbb{E} log~\bar \pi_k - \frac{1}{2} \mathbb{E}\bar w_k^T A_k^{-1} \bar w_k + \frac{1}{2} log~det~A_k^{-1} +$$
$$\sum \limits_{i=1}^{m} \mathbb{E}z_{ik}(log~\beta - \frac{\beta}{2} \mathbb{E}(y_i - \bar w_k^T \bar x_i)^2))$$

$$\frac{\partial \mathcal{F}}{\partial A_k^{-1}} = 0 \Rightarrow \Tilde{A_k} = Diag\left(\mathbb{E} (w_k^i)^2\right)$$

$$\frac{\partial \mathcal{F}}{\partial \beta} = 0 \Rightarrow \Tilde{\beta}  =  \frac{\sum \limits_{k=1}^{K} \sum \limits_{i=1}^{m} \frac{1}{2} \mathbb{E}z_{ik} \left(y_i - \bar{w_k}^T \bar{x_i} \right)^2}{\sum \limits_{k=1}^{K} \sum \limits_{i=1}^{m} \mathbb{E}z_{ik}}$$

\section{Вывод}
В ходе работы было изучено введение в байесовскую статистику. Далее была построена и обучена линейная мультимодель а также протестирована на синтетической выборке. Полученные матожидания векторов весов моделей совпадают с истинными векторами весов в пределах погрешности. В дальнейшем планируется построить алгоритм моделирования изменений параметров моделей во времени. А также учитывать скачкообразные изменения в модели.\\


\begin{thebibliography}{99}
\bibitem{aduenko}
    \BibAuthor{Адуенко\;А.\,А.}
    \BibTitle{Выбор мультимоделей в задачах классификации}, 2017.

\bibitem{bishop}
    \BibAuthor{Bihsop\;C.M.}
    \BibTitle{Pattern recognition and machine learning}~
    \BibJournal{Berlin: Springer}, 2006.

\bibitem{McKay}
    \BibAuthor{MacKay\;D.J.C}
    \BibTitle{The evidence framework applied to classification networks}~
    \BibJournal{Neural computation 4.5}, 1992. Pp.\,720--736.

\bibitem{gelman}
    \BibAuthor{Gelman\;A.}
    \BibTitle{Bayesian data analysis}
    \BibJournal{Florida: Chapman and Hall/CRC}, 2013.

\bibitem{sampleSize}
    \BibAuthor{Motrenko\;A.,Strijov\;V.,Weber\;G.-W.}
    \BibTitle{Sample size determination for logistic regression.}
    \BibJournal{Journal of Computational and Applied Mathematics}, 2014. Vol.\,255, Pp.\,743-752.
\end{thebibliography}

\end{document}
